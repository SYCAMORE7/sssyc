## 该文档内容为本人学习过程中整理的笔记内容，仅限个人参考，不定期更新。

## 深度学习-感知机

感知机是由美国学者FrankRosenblatt在1957年提出来的。感知机作为深度学习的起源算法。感知机的构造是神经网络和深度学习的一种重要思想。

感知机接收多个输入信号，输出一个信号。假设x1,x2是输入信号，y是输出信号，w1,w2是权重。当（x1w1,x2w2）满足界限值时，y的输出为1。则此神经元被激活。这里的界限值称为阈值。

（x1w1+x2w2）是否大于阈值？是则y输出1/否则y输出0

一个神经元是否被激活主要取决于输入信号值x乘以各自权重w之和是否大于等于阈值。

机器学习的课题就是将这个决定参数值的过程交给计算机自动进行。学习是确定合适参数的过程，我们要做的就是思考感知机的构造（模型）并把训练数据交给计算机。

在上述基础上我们再增加偏置，则原来的公式变化为：

（x1w1+x2w2+b）是否大于阈值？是则y输出1/否则y输出0

这里的w1和w2为权重，b为偏置。

权重是控制输入信号的重要性的参数，偏置是调整神经元被激活的容易程度（神经元被激活则y输出1）的参数

如果偏置为-0.1则需要输入信号的加权总和超过0.1，神经元才会被激活。

如果偏置为-20则需要输入信号的加权综合超过20，神经元才会被激活。

感知机有其自身的局限性，感知机的局限性就在于它只能表示由一条直线分割的空间，也就是线性空间问题，对于非线性空间问题感知机无法表示。

但是感知机可以通过增加叠加层的方法来解决非线性空间问题。

单层感知机无法分离非线性空间，但是我们可以通过组合感知机（叠加层）来进行对非线性空间的分离问题。

1、第零层的两个神经元接收输入信号，并将信号发送至第一层的神经元。

2、第一层的神经元将信号发送至第二层的神经元，第二层的神经元输出y.

## 深度学习-神经网络（1）

感知机对于复杂的函数，也能够有可能表示。但是感知机在设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是只能人工设定。

但是本文讲述的神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。

神经网络和感知机有很多的共同点。在这里我们主要从两者的差异来介绍神经网络的结构。

我们以最简单的三层神经网络来介绍，一般最左侧为输入层，最右侧为输出层，中间的为中间层有时也被成为隐藏层。

感知机接收x1和x2两个输入信号，输出y。如果用数学公式可以表示为

(b + w1x1 + w2x2<=0)   y=0

(b + w1x1 + w2x2>0)     y=1    （公式一）

b被成为偏置的参数，用于控制神经元被激活的容易程度；而w1和w2是表示各个信号的权重的参数，用于控制输入信号的重要性。

我们将上述公式化简为更加简洁的形式

y=h(b + w1x1 + w2x2)   （公式二）

将y用h(x)的函数表示，当超过0时输出1，否则输出0

h(x)=0(x<=0)

h(x)=1(x>0)      （公式三）

上述h(x)函数会将输入信号的总和转化为输出信号，这种函数被成为激活函数。

我们将公式二分为两部分来处理，先计算输入信号的加权总和，然后用激活函数来转换这一总和得到下述公式

a=b + w1x1 + w2x2    （公式四）

y=h(a)                    （公式五）

首先公式四计算加权输入信号和偏置的总和记为a。然后公式五用h()函数将a转化为输出y。

如下图：

1—b————↓

x1—w1—→   a—h()—→y

x2—w2———↑

激活函数是连接感知机和神经网络的桥梁。

“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数模型。（阶跃函数是指一旦输出超过阈值，就切换输出的函数）

“多层感知机”是指神经网络，指的是使用sigmoid函数等平滑的激活函数的剁成网络。

感知机使用的激活函数是阶跃函数，如果将感知机的阶跃函数换成其他函数就进入了神经网络的世界。

下面介绍神经网络使用的激活函数：

1、sigmoid函数

神经网络中常用的激活函数就是sigmoid函数:

h(x)=1/(1+exp(-x)) （公式六）

exp(-x)表示e的-x次幂。e是纳皮尔常数2.7182...。

如果向函数中输入h(1.0)=0.731...  h(2.0)=0.880..

神经网络中用sigmoid函数作为激活函数，进行信号的转换，转换后的信号被传送到下一个神经元。

上一篇文章介绍的感知机和接下来介绍的神经网络的主要区别就在于这个激活函数。其他方面，比如神经元的多层连接的构造、信号的传递方法等，基本上和感知机是一样的。

下面我们通过和阶跃函数的比较来详细学习作为激活函数的sigmoid函数。

首先两个函数的平滑性不用填，sigmoid函数是一条平滑的曲线，随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。sigmoid函数的平滑性对神经网络的学习具有重要意义。

另一个不同，相对于阶跃函数只能返回0或1，sigmoid函数可以返回0.731...、0.880...等实数（这一点和平滑性有关）。也就是说，感知机中的神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。

阶跃函数和sigmoid函数的共同性质。阶跃函数和sigmoid函数虽然在平滑性上有差异，但是从宏观角度来看可以发现它们有相似的形状。两者结构均是输入小则输出接近0（为0），输入大则输出接近1（为1）。当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值。还有一点是不管输入信号有多小或者多大，输出信号都在0到1之间。

解原函数和sigmoid函数还有其他共同点，就是两者均为非线性函数。sigmoid函数是一条曲线，阶跃函数是一条向阶梯一样的折线，两者都属于非线性的函数。

输入值是输出值常数倍的函数被成为线性函数h(x)=cx（c为常数）。

非线性函数指不像现行函数那样呈现一条直线的函数。

神经网络的激活函数必须使用非线性函数。激活函数不能使用线性函数。

如果神经网络使用线性函数那么加深神经网络的层数就没有意义了。

h(x)=cx作为激活函数

y(x)=h(h(h(x)))对应的3层神经网络，这个运算会变成y(x)=c*c*c*x的乘法运算。可以等效为一次y(x)=ax此时a=c*c*c的乘法运算表示。神经网络使用线性函数时无法发挥多层网络带来的优势。因此为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。

除了上述的阶跃函数和sigmoid函数。还有最近主要使用的ReLU函数

ReLU函数在输入大于0时，输出该值；在输入小于0时，输出0。

## 深度学习-神经网络（2）

本文共1500+字，阅读时间大约在20分钟左右，本文主要介绍了神经网络的简单实现，在文末有代码。

实现神经网络需要运用到部分矩阵知识。

使用省略偏置和激活函数的神经网络，只有权重。

x           w        =        y

2          2*3                3

实现神经网络时，要注意X、W、Y的形状，特别是X和W的对应维度的元素个数是否一致，这一点很重要。

X = np.array([1,2])

X.shape

(2,)

W = np.array([[1,3,5],[2,4,6]])

print(W)

[[1 3 5] [2 4 6]]

W.shape

(2,3)

Y=np.dot(X,W)

print(Y)

[5 11 17]

上述部分是使用python的np.dot(多维数组的点积)，可以一次性计算出Y的结果。

这意味着，即便Y的元素个数为100或1000,也可以通过一次运算就计算出结果。

如果不使用np.dot，就必须单独计算Y的每一个元素（或者说必须使用for语句），

因此，通过矩阵的成绩一次性完成计算的技巧，在实现的层面上可以说是非常重要的。

3层神经网络的实现

实现从输入到输出的（前向）处理。在代码方面使用NumPy数组，可以用很少的代码完成神经网络的前向处理。

3层神经网络：输入层（第0层）有2个神经元，第一层隐藏层（第一层）有三个神经元，第二个隐藏层（第二层）有两个神经元，输出层（第三层）有两个神经元

定义符号：W(1)12    (1)第一层的权重  1（后一层的第一个神经元）2（前一层的第二个神经元）

第一层加权和可以表示成下面的式

A(1)=XW(1)+B(1)

其中A(1)、X、B(1)、W(1)如下所示。

A(1)=()

A(1)=(a(1)1    a(1)2    a(1)3)

X=(x1,x2)    #第一个输入和第二个输入

B(1)=(b(1)1    b(1)2    b(1)3)

W(1)=(w(1)11    w(1)21    w(1)31

w(1)12    w(1)22    w(1)32)

python 代码

X = np.array([1.0,0.5])

W1=np.array([0.1,0.3,0.5],[0.2,0.4,0.6])

B1=np.array([0.1,0.2,0.3])

A1=np.dot(X,W1)  + B1

这里W1和X的对应维度的元素个数也保持一致。

隐藏层的加权和（加权信号和偏置的总和）用a表示，被激活函数转换后的信号用z表示。此外，图中h()表示激活函数，这里我们使用的是sigmoid函数。用python,代码如下表示。

Z1 = sigmoid(A1)

sigmoid函数会接收到数组并返回元素个数相同的NumPy数组。

下面，我们来实现第一层到第二层的信号传递

W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])

B2 = np.array([0.1,0.2])

A2 = np.dot(Z1,W2) + B2

Z2 = sigmoid(A2)

除了第一层的输出(Z1)变成了第二层的输入这一点意外，这个实现和刚才的diamante完全相同。由此可知，通过使用NumPy数组，可以将层到层的信号传递过程简单地写出来

最后是第2层到输出层的信号传递。输出层的实现也和之前的实现基本相同。不过，最后的激活函数和之前的隐藏层有所不同。

def identity_function(x):

return x

W3 = np.array([[0.1,0.3],[0.2,0.4]])

B3 = np.array([0.1,0.2])

A3 = np.dot(Z2,W3)+B3

Y= identity_function(A3) #或者Y = A3

这里我们定义了identity_function()函数（也称为“恒等函数”），并将其他作为输出层的激活函数。恒等函数会将输入按原样输出。出书层的激活函数用sigma()表示，不同于隐藏层的激活函数h()

输出层所用的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元分类问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。

按照神经网络的实现惯例，只把权重记为大写字母W1，其他的（偏置或者中间结果等）都用小写字母表示。

def init_network{};

network = {}

network['W1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])

network['b1'] = np.array([0.1,0.2,0.3])

network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])

network['b2'] = np.array([0.1,0.2])

network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])

network['b3'] = np.array([0.1,0.2])

return network

def forward(network,x):

    W1,W2,W3 = network['W1'],network['W2'],network['W3']

b1,b2,b3 = network['b1'],network['b2'],network['b3']

a1=np.dot(x,W1) + b1

z1=sigmoid(a1)

a2=np.dot(z1,W2) + b2

z2=sigmoid(a2)

a3=np.dot(z2,W3) + b3

y=identity_function(a3)

    return y

network = init_network()

x = np.array([1.0,0.5])

y = forward(network,x)

print(y)#[0.31682708 0.69627909]

这里定义了init_network()和forward()函数.init_network()函数会进行权重和偏置的初始化，并将它们保存在字典变量network中。这个字典变量network中保存了每一层所需的参数（权重和偏置）。forward()函数中则封装了将输入信号转换为输出信号的处理过程。

另外，这里出现了forward(前向)一词，它表示的是从输入到输出方向的传递处理。后面在进行神经网络的训练时，我们将介绍后向（backward,从输出到输入方向）的处理。

至此，神经网络的前向处理的实现就完成了。通过巧妙地使用NumPy多维数组，我们高效的实现神经网络。

## 深度学习-神经网络输出层

本文大约1700+字，大约阅读15分钟，主要介绍神经网络的输出层相关内容。

神经网络可以用在分类问题和回归问题上，不过需要根据情况改变输出层的激活函数。一般而言，回归问题用恒等函数，分类问题用softmax函数。

机器学习的问题大致可以分为分类问题和回归问题。分类问题是数据属于那一类别的问题。比如，区分图像中的人是男性还是女性的问题就是分类问题。而回归问题是根据某个输入预测一个（连续的）数值问题。比如，根据一个人的图像预测这个人的体重问题就属于回归问题。

恒等函数和softmax函数

恒等函数会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出。因此，在输出层使用恒等函数时，输入信号会原封不动地被输出。

分类问题中使用的softmax函数可以用下面的式表示。

yk=exp(ak)/(从1到n求和exp(ai))    公式一

exp(x)是表示e的x次幂的指数函数（e是纳皮尔常数2.7182...）。

公式一ibaoshi假设输出层共有n个神经元，计算第k个神经元的输出yk。softmax函数的分子是输入信号ak的指数函数，分母是所有输入信号的指数函数之和。

输入层的各个神经元都受所有输入信号的影响。

现在我们来实现softmax函数。在这个过程中，我们将使用Python解释器逐一确认结果

a = np.array(0.3,2.9,4.0)

exp_a = np.exp(a)

print(exp_a)

[    1.34985881    18.17414537    54.59815003]

sum_exp_a = np.sum(exp_a)

print(sum_exp_a)

74.1221542102

y=exp_a/sum_exp_a

print(y)

[0.01821127    0.24519181    0.73659691]

一会还会使用softmax函数

def    softmax(a):

exp_a = np.exp(a)

sum_exp_a = np.sum(exp_a)

y = exp_a / sum_exp_a

return y

上面的softmax函数的实现虽然正确描述了公式一，但是在计算机的运算上有一定的缺陷，这个缺陷是溢出问题。softmax函数的实现要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。如果在这些超大值之间进行除法运算，结果会出现不确定的情况。

超大值无法表示的问题，这个问题被称为溢出，进行计算机计算时需要注意。

可以对公式一分子和分母上都乘以C这个任意的常数（计算结果不变）。然后，把C移动到指数函数exp中，标记为logC。最后把logC替换成另一个符号C'。

在进行softmax的指数函数的运算时，加上（或者减去）某个常数并不会改变运算结果。这里的C'可以使用任何值，但是为了防止溢出，一般会使用输入信号的最大值。

softmax函数的特征

使用softmax()函数，可以按照如下方式就散神经网络的输出。

a = np.array([0.3,2.9,4.0])

y = softmax(a)

print(y)

[0.01821127    ​0.24519181    ​0.73659691]

np.sum(y)

1.0

如上所示，softmax函数的输出是0.0到1.0之间的实数。并且，softmax函数的输出值的总和为1。输出总和为1是softmax函数的一个重要性质。因为这个性质，我们可以把softmax函数的输出解释为“概率”。

上面的例子可以解释为y[0]概率为1.8%，y[1]的概率为24.5%，y[2]的概率为73.7%。有74%的概率是第二个类别，25%的概率是第一个类别。使用softmax函数我们可以用概率的方法解决问题。

这里需要注意，即便使用了softmax函数，各个元素之间的大小关系也不会改变。因为指数函数(y=exp(x))是单调递增函数。

一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。神经网络在进行分类时，输出层的softmax函数可以省略。在实际问题中，由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数一般会被省略。

输出层的神经元数目需要根据待解决的问题来决定。对于分类问题，输出层的神经元数目一般设定为类别的数量。



