## 该文档内容为本人学习过程中整理的笔记内容，仅限个人参考，不定期更新。

#### 深度学习-感知机

感知机是由美国学者FrankRosenblatt在1957年提出来的。感知机作为深度学习的起源算法。感知机的构造是神经网络和深度学习的一种重要思想。

感知机接收多个输入信号，输出一个信号。假设x1,x2是输入信号，y是输出信号，w1,w2是权重。当（x1w1,x2w2）满足界限值时，y的输出为1。则此神经元被激活。这里的界限值称为阈值。

（x1w1+x2w2）是否大于阈值？是则y输出1/否则y输出0

一个神经元是否被激活主要取决于输入信号值x乘以各自权重w之和是否大于等于阈值。

机器学习的课题就是将这个决定参数值的过程交给计算机自动进行。学习是确定合适参数的过程，我们要做的就是思考感知机的构造（模型）并把训练数据交给计算机。

在上述基础上我们再增加偏置，则原来的公式变化为：

（x1w1+x2w2+b）是否大于阈值？是则y输出1/否则y输出0

这里的w1和w2为权重，b为偏置。

权重是控制输入信号的重要性的参数，偏置是调整神经元被激活的容易程度（神经元被激活则y输出1）的参数

如果偏置为-0.1则需要输入信号的加权总和超过0.1，神经元才会被激活。

如果偏置为-20则需要输入信号的加权综合超过20，神经元才会被激活。

感知机有其自身的局限性，感知机的局限性就在于它只能表示由一条直线分割的空间，也就是线性空间问题，对于非线性空间问题感知机无法表示。

但是感知机可以通过增加叠加层的方法来解决非线性空间问题。

单层感知机无法分离非线性空间，但是我们可以通过组合感知机（叠加层）来进行对非线性空间的分离问题。

1、第零层的两个神经元接收输入信号，并将信号发送至第一层的神经元。

2、第一层的神经元将信号发送至第二层的神经元，第二层的神经元输出y.

#### 深度学习-神经网络（1）

感知机对于复杂的函数，也能够有可能表示。但是感知机在设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是只能人工设定。

但是本文讲述的神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。

神经网络和感知机有很多的共同点。在这里我们主要从两者的差异来介绍神经网络的结构。

我们以最简单的三层神经网络来介绍，一般最左侧为输入层，最右侧为输出层，中间的为中间层有时也被成为隐藏层。

感知机接收x1和x2两个输入信号，输出y。如果用数学公式可以表示为

(b + w1x1 + w2x2<=0)   y=0

(b + w1x1 + w2x2>0)     y=1    （公式一）

b被成为偏置的参数，用于控制神经元被激活的容易程度；而w1和w2是表示各个信号的权重的参数，用于控制输入信号的重要性。

我们将上述公式化简为更加简洁的形式

y=h(b + w1x1 + w2x2)   （公式二）

将y用h(x)的函数表示，当超过0时输出1，否则输出0

h(x)=0(x<=0)

h(x)=1(x>0)      （公式三）

上述h(x)函数会将输入信号的总和转化为输出信号，这种函数被成为激活函数。

我们将公式二分为两部分来处理，先计算输入信号的加权总和，然后用激活函数来转换这一总和得到下述公式

a=b + w1x1 + w2x2    （公式四）

y=h(a)                    （公式五）

首先公式四计算加权输入信号和偏置的总和记为a。然后公式五用h()函数将a转化为输出y。

如下图：

1—b————↓

x1—w1—→   a—h()—→y

x2—w2———↑

激活函数是连接感知机和神经网络的桥梁。

“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数模型。（阶跃函数是指一旦输出超过阈值，就切换输出的函数）

“多层感知机”是指神经网络，指的是使用sigmoid函数等平滑的激活函数的剁成网络。

感知机使用的激活函数是阶跃函数，如果将感知机的阶跃函数换成其他函数就进入了神经网络的世界。

下面介绍神经网络使用的激活函数：

1、sigmoid函数

神经网络中常用的激活函数就是sigmoid函数:

h(x)=1/(1+exp(-x)) （公式六）

exp(-x)表示e的-x次幂。e是纳皮尔常数2.7182...。

如果向函数中输入h(1.0)=0.731...  h(2.0)=0.880..

神经网络中用sigmoid函数作为激活函数，进行信号的转换，转换后的信号被传送到下一个神经元。

上一篇文章介绍的感知机和接下来介绍的神经网络的主要区别就在于这个激活函数。其他方面，比如神经元的多层连接的构造、信号的传递方法等，基本上和感知机是一样的。

下面我们通过和阶跃函数的比较来详细学习作为激活函数的sigmoid函数。

首先两个函数的平滑性不用填，sigmoid函数是一条平滑的曲线，随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。sigmoid函数的平滑性对神经网络的学习具有重要意义。

另一个不同，相对于阶跃函数只能返回0或1，sigmoid函数可以返回0.731...、0.880...等实数（这一点和平滑性有关）。也就是说，感知机中的神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。

阶跃函数和sigmoid函数的共同性质。阶跃函数和sigmoid函数虽然在平滑性上有差异，但是从宏观角度来看可以发现它们有相似的形状。两者结构均是输入小则输出接近0（为0），输入大则输出接近1（为1）。当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值。还有一点是不管输入信号有多小或者多大，输出信号都在0到1之间。

解原函数和sigmoid函数还有其他共同点，就是两者均为非线性函数。sigmoid函数是一条曲线，阶跃函数是一条向阶梯一样的折线，两者都属于非线性的函数。

输入值是输出值常数倍的函数被成为线性函数h(x)=cx（c为常数）。

非线性函数指不像现行函数那样呈现一条直线的函数。

神经网络的激活函数必须使用非线性函数。激活函数不能使用线性函数。

如果神经网络使用线性函数那么加深神经网络的层数就没有意义了。

h(x)=cx作为激活函数

y(x)=h(h(h(x)))对应的3层神经网络，这个运算会变成y(x)=c*c*c*x的乘法运算。可以等效为一次y(x)=ax此时a=c*c*c的乘法运算表示。神经网络使用线性函数时无法发挥多层网络带来的优势。因此为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。

除了上述的阶跃函数和sigmoid函数。还有最近主要使用的ReLU函数

ReLU函数在输入大于0时，输出该值；在输入小于0时，输出0。

