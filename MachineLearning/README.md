## 该文档内容为本人学习过程中整理的笔记内容，仅限个人参考，不定期更新。
## 基础

1、模型拟合

    （1）欠拟合：
    模型训练不足，无法正确识别出特征
  
    解决办法：增加训练轮数。
    
    （2）过拟合：

    模型训练过度，将不是特征的部分识别成特征
    
    解决办法：剪枝处理
    
2、模型验证方法：交叉验证

3、类别不平衡问题：指的是分类任务中不同类别的训练样例数目差别较大的情况

    对样例中多里数目样例进行欠采样，丢掉一些多的样例，让正反样例数目相近，在进行学习

    对样例汇总数目少的样例进行过采样，增加一些样例少的一方的样例，在进行学习

    直接基于原始训练集进行学习，但在用训练好的分类器进行预测时进行阈值移动

    （1）过采样：过采样法不能简单地对初始样本进行重复采样，否则容易导致过拟合
     
    过采样代表性的算法：SMOTE算法，通过对训练集中的样例进行插值来产生额外的正例

    （2）欠采样：欠采样法若随机丢弃，可能丢失一些重要信息。

    欠采样法的代表性算法：EasyEnsemble算法，通过集成学习机制，将样例划分若干集合供学习器使用。
    
    对每个学习器欠采样，但是对全局不产生影响。
   
4、剪枝
    
    剪枝是决策树处理过拟合的主要手段
    
    预剪枝：在决策树生成过程中进行剪枝，对不能提高泛化能力的部分进行剪除。
    
    优点：减少时间消耗，避免过拟合
    
    缺点：可能导致欠拟合
    
    后剪枝：在决策树生成后对树进行剪枝，一般来说效果好于预剪枝，但是由于需要决策树完全生成以后然后再进行剪枝处理，
    
    所以时间成本特别高。
    
    优点：相对于预剪枝欠拟合可能性小
    
    缺点：时间成本特别高
    
5、特殊数据处理

    特殊数据主要是指连续数据和缺失数据
    
    最常用的连续数据处理方法：连续属性离散化方法（其中最简单的使用二分法对连续属性进行处理）
    
    对于缺失数据的处理方法：对于在某条属性上缺失的值，可以根据该条属性上未缺失的值来判断该属性值得优劣
    
6、陷入局部最小值时解决办法：
     
    随机结构：随机赋予初始参数，从多个初始模型同事优化
        
    模拟退火：按照次小值进行优化
        
    随机梯度下降：向随机梯度方向进行优化
        
    遗传算法：X

## 线性模型

   线性回归:

   1、线性模型：回归、分类
     
   2、非线性模型=线性+层级结构/高维映射
     
   3、线性模型试图学得一个通过属性的线性组合来预测函数
   
   f(x)=w1x1+w2x2+w3x3...wdxd+b
   
   4、线性模型简单易于建模
   
   5、由于权重w能够直观的表示各个因素的重要性，因此线性模型有很好的可解释性。
   
   6、线性回归：线性回归试图学得一个线性模型以尽可能准确地预测实值输出标记。
   
   7、对于离散属性，若属性值间存在序关系，可以通过连续化将其转化为连续值
   
   8、均方误差是回归任务中最常用的性能度量
   
   9、均方误差对应常用的欧几里得距离（欧式距离）基于均方误差最小化来进行模型求解的方法成为最小二乘法
   
   10、在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离最小。
   
   11、求解权重w和偏差b最小化的过程成为线性回归模型的最小二乘参数估计
   
   12、线性模型进行分类任务：找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来
   
   13、对数几率函数是一种Sigmoid函数
   
   14、线性回归模型的预测结果去逼近真实标记的对数几率，其对应的模型称为对数几率回归，虽然名字有回归，实际上却是一种分类学习方法
   
   15、对数几率回归优点：直接对分类可能性进行建模，无需事先假设数据分布，这样就能避免假设分布不准确带来的问题；它不仅仅预测出类别，而是可以得到近似概率预测，这对许多需要利用概率辅助决策的任务很有用；对数回归求解的目标函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可以直接用于求取最优解
   
   16、对于高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法有：梯度下降法，牛顿法都可以求出最优解
   
   17、线性判别分析是一种经典的线性学习方法
   
   18、线性判别分析思想是给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、不同样例尽可能远离
   
   19、对新样本进行分类时，将其投影到同样的直线上，在根据投影点的位置来确定新样本的类别
   
   20、为了使同类样例尽可能近，需要同类投影点的协方差最小。
   
   21、多分类学习的基本思路就是拆解法，将多分类问题拆解为多个二分类问题进行求解
   
   22、多分类问题的拆解策略有一对一、一对多、多对多。
   

## 决策树

1、决策树是基于递归形成的

    递归返回条件：类别相同；属性相同或属性集为空；样本集为空

2、决策树使用分治算法思路

3、决策树通过树形结构解决多分类问题

4、决策树划分方法
    
    决策树使用有意义的信息增益最大的属性为划分属性

    决策树算法使用增益率来选择最优划分属性
    
    决策树使用基尼指数来选择划分属性
    
 5、多变量决策树
    
    如果将每个属性当做坐标空间中的坐标轴，那么n个属性描述的样本对应n维空间里面的一个坐标点
    
    决策树形成的分类边界由若干个与坐标轴平行的分段组成（轴平行）
    
## 神经网络

1、BP算法作为经典方法

2、BP算法容易产生过拟合

3、全局最小和局部极小：全局的最小值，局部范围的极小值（不一定是全局最小值）
        
4、常用激活函数：sigmoid函数、ReLU函数（小于零时，输出零；大于零时，输出原值）

## 支持向量机

1、支持向量划分超平面解决分类问题

2、核函数是通过将低维函数映射到高维空间实现划分

3、软间隔：支持向量划分超平面时，允许部分值存在错误。

## 贝叶斯分类器

1、贝叶斯分类器

    分类器考虑概率分布和错误选择时的代价

2、朴素贝叶斯分类器

    在贝叶斯分类器的基础上，默认各个属性之间不存在关联关系

3、半朴素贝叶斯分类器

    在朴素贝叶斯的基础上，允许属性和一个属性之间有关联关系

4、贝叶斯网（信念网）

    用有向无环图表示关联关系，用条件概率表来描述属性的联合概率分布。
    
5、EM算法

    E步：模型参数已知，可以根据训练数据推断出最优隐变量的值
    
    M步：隐变量值已知，可以对模型参数做极大似然估计


